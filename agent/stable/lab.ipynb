{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym\n",
    "import os\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "# helpers\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete, Tuple\n",
    "\n",
    "# Stable-baseline\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# from environment import KubernetesEnv\n",
    "\n",
    "# Custom\n",
    "from typing import Optional\n",
    "from database.influxdb import InfluxDB\n",
    "import logging\n",
    "from kubernetes import client, config\n",
    "from utils import setup_logger\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger, log_dir = setup_logger(\n",
    "    \"kubernetes_agent\", log_level=os.getenv(\"LOG_LEVEL\", \"INFO\"), log_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353220fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_pods_ready(\n",
    "    prometheus: PrometheusConnect,\n",
    "    namespace: str,\n",
    "    deployment_name: str,\n",
    "    timeout: int,\n",
    "    wait_time: int,\n",
    "    logger: logging.Logger,\n",
    "):\n",
    "    start_time = time.time()\n",
    "\n",
    "    scope_ready = f\"\"\"\n",
    "        (kube_pod_status_ready{{namespace=\"{namespace}\", condition=\"true\"}} == 1)\n",
    "        and on(pod)\n",
    "        (\n",
    "          label_replace(\n",
    "            kube_pod_owner{{namespace=\"{namespace}\", owner_kind=\"ReplicaSet\"}},\n",
    "            \"replicaset\", \"$1\", \"owner_name\", \"(.*)\"\n",
    "          )\n",
    "          * on(namespace, replicaset) group_left(owner_name)\n",
    "            kube_replicaset_owner{{\n",
    "              namespace=\"{namespace}\", owner_kind=\"Deployment\", owner_name=\"{deployment_name}\"\n",
    "            }}\n",
    "        )\n",
    "    \"\"\"  # noqa: E501\n",
    "    q_desired = f\"\"\"\n",
    "    scalar(\n",
    "      sum(\n",
    "        kube_deployment_spec_replicas{{namespace=\"{namespace}\",\n",
    "        deployment=\"{deployment_name}\"}}\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "    q_ready = f\"\"\"\n",
    "      scalar(sum({scope_ready}))\n",
    "    \"\"\"\n",
    "\n",
    "    while time.time() - start_time < timeout:\n",
    "        desired_result = prometheus.custom_query(q_desired)\n",
    "        desired = desired_result[1]\n",
    "        ready_result = prometheus.custom_query(query=q_ready)\n",
    "        ready = ready_result[1]\n",
    "        if ready == desired:\n",
    "            time.sleep(wait_time)\n",
    "            return True, desired, ready\n",
    "        logger.debug(\n",
    "            f\"Waiting for pods to be ready: {ready}/{desired}\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "    time.sleep(wait_time)\n",
    "    return False, desired, ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba23d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metrics_query(\n",
    "    namespace: str,\n",
    "    deployment_name: str,\n",
    "    interval: int = 15,\n",
    "    desired_replicas: int | None = None,\n",
    "    quantile: float = 0.90,\n",
    "    endpoints_method: list[tuple[str, str]] = ((\"/\", \"GET\"), (\"/docs\", \"GET\")),\n",
    ") -> tuple[str, str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Build pod-scoped queries and cap to the youngest desired pods.\n",
    "\n",
    "    We use topk on pod start time to keep only the newest N pods (desired replicas),\n",
    "    so older pods that are still Ready after a scale-down do not contribute.\n",
    "    \"\"\"\n",
    "    # Default to a reasonable cap if desired_replicas is None\n",
    "    pod_window = max(1, desired_replicas or 50)\n",
    "\n",
    "    pod_filter = f\"\"\"\n",
    "        topk({pod_window},\n",
    "          kube_pod_start_time{{\n",
    "            namespace=\"{namespace}\",\n",
    "            pod=~\"{deployment_name}-.*\"\n",
    "          }}\n",
    "          * on(pod) group_left()\n",
    "            (kube_pod_status_ready{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                condition=\"true\"\n",
    "            }} == 1)\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    cpu_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            rate(container_cpu_usage_seconds_total{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                container!=\"\",\n",
    "                container!=\"POD\"\n",
    "            }}[{interval}s])\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    memory_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            container_memory_working_set_bytes{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                container!=\"\",\n",
    "                container!=\"POD\"\n",
    "            }}\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    cpu_limits_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            kube_pod_container_resource_limits{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                resource=\"cpu\",\n",
    "                unit=\"core\"\n",
    "            }}\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    # Query for memory limits\n",
    "    memory_limits_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            kube_pod_container_resource_limits{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                resource=\"memory\",\n",
    "                unit=\"byte\"\n",
    "            }}\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    response_time_query = []\n",
    "    for endpoint, method in endpoints_method:\n",
    "        response_time_query.append(f\"\"\"\n",
    "                1000 *\n",
    "                histogram_quantile(\n",
    "                {quantile},\n",
    "                sum by (le) (\n",
    "                    rate(http_request_duration_seconds_bucket{{\n",
    "                    namespace=\"{namespace}\",\n",
    "                    pod=~\"{deployment_name}-.*\",\n",
    "                    method=\"{method}\",\n",
    "                    path=\"{endpoint}\"\n",
    "                    }}[{interval}s])\n",
    "                )\n",
    "                )\n",
    "            \"\"\"\n",
    "        )\n",
    "    return (\n",
    "        cpu_query,\n",
    "        memory_query,\n",
    "        cpu_limits_query,\n",
    "        memory_limits_query,\n",
    "        response_time_query,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98efa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics(cpu_usage, memory_usage, cpu_limits, memory_limits, response_times, max_response_time):\n",
    "    cpu_percentages = []\n",
    "    memory_percentages = []\n",
    "\n",
    "    cpu_limits_by_pod = {}\n",
    "    memory_limits_by_pod = {}\n",
    "    for item in cpu_limits:\n",
    "        pod = item[\"metric\"][\"pod\"]\n",
    "        limit = float(item[\"value\"][1])\n",
    "        cpu_limits_by_pod[pod] = limit\n",
    "\n",
    "    for item in memory_limits:\n",
    "        pod = item[\"metric\"][\"pod\"]\n",
    "        limit = float(item[\"value\"][1])\n",
    "        memory_limits_by_pod[pod] = limit\n",
    "\n",
    "    for result in cpu_usage:\n",
    "        pod_name = result[\"metric\"].get(\"pod\")\n",
    "        limit = float(cpu_limits_by_pod.get(pod_name))\n",
    "        rate_cores = float(result[\"value\"][1])\n",
    "        cpu_percentage = (rate_cores / limit) * 100\n",
    "        cpu_percentages.append(cpu_percentage)\n",
    "\n",
    "    for result in memory_usage:\n",
    "        pod_name = result[\"metric\"].get(\"pod\")\n",
    "        limit = float(memory_limits_by_pod.get(pod_name))\n",
    "        usage_bytes = float(result[\"value\"][1])\n",
    "        memory_percentage = (usage_bytes / limit) * 100\n",
    "        memory_percentages.append(memory_percentage)\n",
    "\n",
    "    response_time = np.mean(response_times) if response_times else 0.0\n",
    "    response_time_percentage = (response_time / max_response_time) * 100.0\n",
    "    response_time_percentage = min(response_time_percentage, 1000.0)\n",
    "\n",
    "    return np.mean(cpu_percentages), np.mean(memory_percentages), response_time_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e35dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    prometheus: PrometheusConnect,\n",
    "    namespace: str,\n",
    "    deployment_name: str,\n",
    "    interval: int,\n",
    "    replica: int,\n",
    "    max_response_time: float,\n",
    "):\n",
    "    (\n",
    "        cpu_query,\n",
    "        memory_query,\n",
    "        cpu_limits_query,\n",
    "        memory_limits_query,\n",
    "        response_time_query,\n",
    "    )= _metrics_query(\n",
    "        namespace,\n",
    "        deployment_name,\n",
    "        interval=interval,\n",
    "        desired_replicas=replica,\n",
    "    )\n",
    "    cpu_usage_results = prometheus.custom_query(cpu_query)\n",
    "    memory_usage_results = prometheus.custom_query(memory_query)\n",
    "    cpu_limits_results = prometheus.custom_query(cpu_limits_query)\n",
    "    memory_limits_results = prometheus.custom_query(memory_limits_query)\n",
    "\n",
    "    response_time_results = []\n",
    "    for query in response_time_query:\n",
    "        response = prometheus.custom_query(query)\n",
    "        if not response:\n",
    "            response_time_results.append(0.0)\n",
    "            continue\n",
    "\n",
    "        response_time_results.append(float(response[0][\"value\"][1]))\n",
    "\n",
    "    cpu_percentages, memory_percentages, response_time_percentage = process_metrics(\n",
    "        cpu_usage_results,\n",
    "        memory_usage_results,\n",
    "        cpu_limits_results,\n",
    "        memory_limits_results,\n",
    "        response_time_results,\n",
    "        max_response_time,\n",
    "    )\n",
    "\n",
    "    return cpu_percentages, memory_percentages, response_time_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e706895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KubernetesEnv(Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_replicas: int,\n",
    "        max_replicas: int,\n",
    "        iteration: int,\n",
    "        namespace: str,\n",
    "        deployment_name: str,\n",
    "        min_cpu: float,\n",
    "        min_memory: float,\n",
    "        max_cpu: float,\n",
    "        max_memory: float,\n",
    "        max_response_time: float,\n",
    "        timeout: int,\n",
    "        wait_time: int,\n",
    "        logger: Optional[logging.Logger],\n",
    "        influxdb: Optional[InfluxDB],\n",
    "        prometheus_url: str,\n",
    "        metrics_interval: int,\n",
    "        metrics_quantile: float,\n",
    "        max_scaling_retries: int,\n",
    "        weight_response_time: float,\n",
    "        weight_cost: float,\n",
    "        metrics_endpoints_method: list[tuple[str, str]] = (\n",
    "            (\"/cpu\", \"GET\"),\n",
    "            (\"/memory\", \"GET\"),\n",
    "        ),\n",
    "    ):\n",
    "        config.load_kube_config()\n",
    "        self.api = client.AppsV1Api()\n",
    "        self.namespace = namespace\n",
    "        self.deployment_name = deployment_name\n",
    "        self.prometheus = PrometheusConnect(\n",
    "            url=prometheus_url,\n",
    "            disable_ssl=True,\n",
    "        )\n",
    "        self.timeout = timeout\n",
    "        self.wait_time = wait_time\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.metrics_quantile = metrics_quantile\n",
    "        self.metrics_endpoints_method = metrics_endpoints_method\n",
    "        self.logger = logger\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(\n",
    "            low=np.array([0.0, 0.0, 0.0, -2.0, -2.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 1.0, 1.0, 2.0, 2.0, 3.0], dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.iteration = iteration\n",
    "        self.iteration_init = iteration\n",
    "        self.min_replicas: int = min_replicas\n",
    "        self.max_replicas: int = max_replicas\n",
    "        self.range_replicas: int = max(1, self.max_replicas - self.min_replicas)\n",
    "        self.max_response_time: float = max_response_time\n",
    "        self.min_cpu: float = min_cpu\n",
    "        self.min_memory: float = min_memory\n",
    "        self.max_cpu: float = max_cpu\n",
    "        self.max_memory: float = max_memory\n",
    "        self.influxdb = influxdb\n",
    "        self.max_scaling_retries = max_scaling_retries\n",
    "\n",
    "        self.weight_response_time = weight_response_time\n",
    "        self.weight_cost = weight_cost\n",
    "\n",
    "        # Calculate max_response_penalty dynamically to ensure reward range [-1, 1]\n",
    "        # max penalty needed = 2.0 (so 1.0 - 2.0 = -1.0)\n",
    "        # When RT is bad, cost_weight_multiplier = 0, so only RT penalty applies\n",
    "        # weight_response_time * max_response_penalty = 2.0\n",
    "        self.max_response_penalty = 2.0 / self.weight_response_time\n",
    "\n",
    "        self.observations = np.array(\n",
    "            [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        self.last_reward = 0.0\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.iteration -= 1\n",
    "        replica = int(action * self.range_replicas // 100 + self.min_replicas)\n",
    "\n",
    "        cpu, memory, response_time = self.scale(replica)\n",
    "\n",
    "\n",
    "        cpu_relative, memory_relative, cpu_distance, memory_distance = self.calculate_distance(cpu, memory)\n",
    "\n",
    "        reward = self.calculate_reward(action=action, response_time=response_time)\n",
    "        self.last_reward = reward\n",
    "\n",
    "        self.observations = self.observation(action=action, response_time=response_time, cpu_relative=cpu_relative, memory_relative=memory_relative, cpu_distance=cpu_distance, memory_distance=memory_distance)\n",
    "\n",
    "\n",
    "        if self.iteration <= 0:\n",
    "            terminated = True\n",
    "            truncated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "            truncated = False\n",
    "\n",
    "        info = {\n",
    "            \"cpu\": cpu,\n",
    "            \"memory\": memory,\n",
    "            \"response_time\": response_time,\n",
    "            \"replicas\": replica,\n",
    "            \"action\": action,\n",
    "            \"cpu_relative\": cpu_relative,\n",
    "            \"memory_relative\": memory_relative,\n",
    "            \"cpu_distance\": cpu_distance,\n",
    "            \"memory_distance\": memory_distance,\n",
    "        }\n",
    "        if self.influxdb :\n",
    "            self.influxdb.write_point(\n",
    "                measurement=\"autoscaling_metrics\",\n",
    "                tags={\n",
    "                    \"namespace\": self.namespace,\n",
    "                    \"deployment\": self.deployment_name,\n",
    "                },\n",
    "                fields={**info},\n",
    "            )\n",
    "\n",
    "        return self.observations, reward, terminated, truncated, info\n",
    "\n",
    "    def scale(self, replica: int) -> None:\n",
    "        attempt = 0\n",
    "        while attempt < self.max_scaling_retries:\n",
    "            attempt += 1\n",
    "            delay = min(1 * (2 ** (attempt - 1)), 10)\n",
    "            try:\n",
    "                self.api.patch_namespaced_deployment_scale(\n",
    "                    name=self.deployment_name,\n",
    "                    namespace=self.namespace,\n",
    "                    body={\"spec\": {\"replicas\": replica}},\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error scaling deployment: {e}\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "        _, _, _ = wait_for_pods_ready(\n",
    "            prometheus=self.prometheus,\n",
    "            deployment_name=self.deployment_name,\n",
    "            namespace=self.namespace,\n",
    "            timeout=self.timeout,\n",
    "            wait_time=self.wait_time,\n",
    "            logger=self.logger,\n",
    "        )\n",
    "        cpu, memory, response_time = get_metrics(\n",
    "            prometheus=self.prometheus,\n",
    "            namespace=self.namespace,\n",
    "            deployment_name=self.deployment_name,\n",
    "            interval=self.metrics_interval,\n",
    "            replica=replica,\n",
    "            max_response_time=self.max_response_time,\n",
    "        )\n",
    "        return cpu, memory, response_time\n",
    "\n",
    "    def calculate_reward(self, action: int, response_time: float) -> float:\n",
    "        RESPONSE_TIME_HIGH_THRESHOLD = 80.0\n",
    "        RESPONSE_TIME_VIOLATION_THRESHOLD = 100.0\n",
    "\n",
    "        if response_time <= RESPONSE_TIME_HIGH_THRESHOLD:\n",
    "            response_time_penalty = 0.0\n",
    "        elif response_time <= RESPONSE_TIME_VIOLATION_THRESHOLD:\n",
    "            response_time_penalty = (response_time - RESPONSE_TIME_HIGH_THRESHOLD) / (\n",
    "                RESPONSE_TIME_VIOLATION_THRESHOLD - RESPONSE_TIME_HIGH_THRESHOLD\n",
    "            )\n",
    "        else:\n",
    "            over = (\n",
    "                response_time - RESPONSE_TIME_VIOLATION_THRESHOLD\n",
    "            ) / RESPONSE_TIME_VIOLATION_THRESHOLD\n",
    "            response_time_penalty = 1.0 + over\n",
    "\n",
    "        response_time_penalty = max(0.0, min(response_time_penalty, self.max_response_penalty))\n",
    "\n",
    "        cost_penalty_raw = action / 99.0\n",
    "\n",
    "        if response_time <= RESPONSE_TIME_HIGH_THRESHOLD:\n",
    "            cost_weight_multiplier = 1.0\n",
    "        elif response_time <= RESPONSE_TIME_VIOLATION_THRESHOLD:\n",
    "            cost_weight_multiplier = 1.0 - (response_time_penalty / self.max_response_penalty)\n",
    "        else:\n",
    "            cost_weight_multiplier = 0.0\n",
    "\n",
    "        effective_cost_penalty = cost_penalty_raw * cost_weight_multiplier\n",
    "\n",
    "        total_penalty = (\n",
    "            self.weight_response_time * response_time_penalty\n",
    "            + self.weight_cost * effective_cost_penalty\n",
    "        )\n",
    "        return 1.0 - total_penalty\n",
    "\n",
    "    def calculate_distance(self,cpu: float, memory: float) -> tuple[float, float]:\n",
    "        cpu_distance = (\n",
    "            (self.min_cpu - cpu) if cpu < self.min_cpu else (cpu - self.max_cpu)\n",
    "        )\n",
    "        cpu_bandwidth = self.max_cpu - self.min_cpu\n",
    "        cpu_normalized = cpu_distance / cpu_bandwidth\n",
    "        cpu_distance = cpu_normalized\n",
    "        cpu_relative = (cpu - self.min_cpu) / cpu_bandwidth\n",
    "\n",
    "\n",
    "        memory_distance = (\n",
    "            (self.min_memory - memory)\n",
    "            if memory < self.min_memory\n",
    "            else (memory - self.max_memory)\n",
    "        )\n",
    "        memory_bandwidth = self.max_memory - self.min_memory\n",
    "        memory_normalized = memory_distance / memory_bandwidth\n",
    "        memory_distance = memory_normalized\n",
    "        memory_relative = (memory - self.min_memory) / memory_bandwidth\n",
    "\n",
    "        return cpu_relative, memory_relative, cpu_distance, memory_distance\n",
    "\n",
    "    def observation(self, action: int, response_time: float, cpu_relative: float, memory_relative: float, cpu_distance: float, memory_distance: float):\n",
    "        action = action / 99.0\n",
    "\n",
    "        cpu_relative = float(np.clip(cpu_relative, 0.0, 1.0))\n",
    "        memory_relative = float(np.clip(memory_relative, 0.0, 1.0))\n",
    "        cpu_distance = float(np.clip(cpu_distance, -2.0, 2.0))\n",
    "        memory_distance = float(np.clip(memory_distance, -2.0, 2.0))\n",
    "        response_time = float(np.clip(response_time / 100.0, 0.0, 3.0))\n",
    "\n",
    "\n",
    "        return np.array(\n",
    "            [\n",
    "                action,\n",
    "                cpu_relative,\n",
    "                memory_relative,\n",
    "                cpu_distance,\n",
    "                memory_distance,\n",
    "                response_time,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "    def render(self) -> None:\n",
    "        def _color(v: float, warn: float, crit: float, reverse: bool = False) -> str:\n",
    "            GREEN, YELLOW, RED = \"\\033[32m\", \"\\033[33m\", \"\\033[31m\"\n",
    "\n",
    "            if reverse:\n",
    "                ok = v <= warn\n",
    "                mid = warn < v <= crit\n",
    "            else:\n",
    "                ok = v < warn\n",
    "                mid = warn <= v < crit\n",
    "                if v >= crit:\n",
    "                    return RED\n",
    "\n",
    "            return GREEN if ok else (YELLOW if mid else RED)\n",
    "\n",
    "        def _clamp(v: float, lo: float = 0.0, hi: float = 100.0) -> float:\n",
    "            return max(lo, min(hi, v))\n",
    "\n",
    "        def _bar(pct: float, width: int = 12) -> str:\n",
    "            pct = _clamp(pct)\n",
    "            filled = round(pct / 100 * width)\n",
    "            return \"█\" * filled + \"░\" * (width - filled)\n",
    "\n",
    "        def _fmt_pct(v: float) -> str:\n",
    "            try:\n",
    "                return f\"{float(v):6.2f}%\"\n",
    "            except Exception:\n",
    "                return f\"{v}\"\n",
    "\n",
    "        action = int(self.observations[0] * 99)\n",
    "        cpu = self.observations[1] * 100.0\n",
    "        mem = self.observations[2] * 100.0\n",
    "        cpu_distance = self.observations[3]\n",
    "        mem_distance = self.observations[4]\n",
    "        rt = self.observations[5] * 100.0\n",
    "\n",
    "        def _dist_color(dist: float) -> str:\n",
    "            GREEN, YELLOW, RED = \"\\033[32m\", \"\\033[33m\", \"\\033[31m\"\n",
    "            if -0.1 <= dist <= 0.1:\n",
    "                return GREEN\n",
    "            elif dist < -0.3 or dist > 0.5:\n",
    "                return RED\n",
    "            return YELLOW\n",
    "\n",
    "        cpu_col = _dist_color(cpu_distance)\n",
    "        mem_col = _dist_color(mem_distance)\n",
    "        rt_col = _color(rt, warn=80, crit=100)\n",
    "\n",
    "        cpu_bar = _bar(cpu)\n",
    "        mem_bar = _bar(mem)\n",
    "        rt_bar = _bar(min(rt, 200.0), width=12)\n",
    "\n",
    "        RESET = \"\\033[0m\"\n",
    "\n",
    "        # line 1\n",
    "        hdr = \"▶ \"\n",
    "        cpu_str = f\"{cpu_col}CPU {_fmt_pct(cpu)} {cpu_bar}{RESET}\"\n",
    "        mem_str = f\"{mem_col}MEM {_fmt_pct(mem)} {mem_bar}{RESET}\"\n",
    "        rt_str = f\"{rt_col}RT {rt:6.1f}% {rt_bar}{RESET}\"\n",
    "        act_str = f\"ACT {action:3d}\"\n",
    "        cpu_dist_str = f\"CPU_D {cpu_distance:+7.3f}\"\n",
    "        mem_dist_str = f\"MEM_D {mem_distance:+7.3f}\"\n",
    "        reward_str = f\"RWD {self.last_reward:+6.3f}\"\n",
    "        self.logger.info(\n",
    "            f\"{' ' * len(hdr)}| {cpu_str} | {mem_str} | {rt_str} | \"\n",
    "            f\"{cpu_dist_str} | {mem_dist_str} | {act_str} | {reward_str} |\"\n",
    "        )\n",
    "\n",
    "    def reset(\n",
    "        self,\n",
    "        *,\n",
    "        seed: int | None = None,\n",
    "        options: dict[str, Any] | None = None,\n",
    "    ):\n",
    "        self.iteration = self.iteration_init\n",
    "        action = random.randint(0, 99)\n",
    "        replica = int(action * self.range_replicas // 100 + self.min_replicas)\n",
    "\n",
    "        cpu, memory, response_time = self.scale(replica)\n",
    "\n",
    "        cpu_relative, memory_relative, cpu_distance, memory_distance = self.calculate_distance(cpu, memory)\n",
    "\n",
    "        self.observations = self.observation(action=action, response_time=response_time, cpu_relative=cpu_relative, memory_relative=memory_relative, cpu_distance=cpu_distance, memory_distance=memory_distance)\n",
    "\n",
    "        info = {\n",
    "            \"cpu\": cpu,\n",
    "            \"memory\": memory,\n",
    "            \"response_time\": response_time,\n",
    "            \"replicas\": replica,\n",
    "            \"action\": action,\n",
    "            \"cpu_relative\": cpu_relative,\n",
    "            \"memory_relative\": memory_relative,\n",
    "            \"cpu_distance\": cpu_distance,\n",
    "            \"memory_distance\": memory_distance,\n",
    "        }\n",
    "\n",
    "        return self.observations, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53faf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "influxdb = InfluxDB(\n",
    "    logger=logger,\n",
    "    url=os.getenv(\"INFLUXDB_URL\", \"http://localhost:8086\"),\n",
    "    token=os.getenv(\"INFLUXDB_TOKEN\", \"my-token\"),\n",
    "    org=os.getenv(\"INFLUXDB_ORG\", \"my-org\"),\n",
    "    bucket=os.getenv(\"INFLUXDB_BUCKET\", \"my-bucket\"),\n",
    ")\n",
    "\n",
    "env = KubernetesEnv(\n",
    "    min_replicas=int(os.getenv(\"MIN_REPLICAS\")),\n",
    "    max_replicas=int(os.getenv(\"MAX_REPLICAS\")),\n",
    "    iteration=int(os.getenv(\"ITERATION\")),\n",
    "    namespace=os.getenv(\"NAMESPACE\"),\n",
    "    deployment_name=os.getenv(\"DEPLOYMENT_NAME\"),\n",
    "    min_cpu=float(os.getenv(\"MIN_CPU\")),\n",
    "    min_memory=float(os.getenv(\"MIN_MEMORY\")),\n",
    "    max_cpu=float(os.getenv(\"MAX_CPU\")),\n",
    "    max_memory=float(os.getenv(\"MAX_MEMORY\")),\n",
    "    max_response_time=float(os.getenv(\"MAX_RESPONSE_TIME\")),\n",
    "    timeout=int(os.getenv(\"TIMEOUT\")),\n",
    "    wait_time=int(os.getenv(\"WAIT_TIME\")),\n",
    "    logger=logger,\n",
    "    influxdb=influxdb,\n",
    "    prometheus_url=os.getenv(\"PROMETHEUS_URL\"),\n",
    "    metrics_endpoints_method=[(\"/\", \"GET\"), (\"/docs\", \"GET\")],\n",
    "    metrics_interval=int(os.getenv(\"METRICS_INTERVAL\")),\n",
    "    metrics_quantile=float(os.getenv(\"METRICS_QUANTILE\")),\n",
    "    max_scaling_retries=int(os.getenv(\"MAX_SCALING_RETRIES\")),\n",
    "    weight_response_time=float(os.getenv(\"WEIGHT_RESPONSE_TIME\")),\n",
    "    weight_cost=float(os.getenv(\"WEIGHT_COST\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9c0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodes = 10\n",
    "# for episode in range(1, episodes+1):\n",
    "#     env.reset()\n",
    "#     terminated = False\n",
    "#     truncated = False\n",
    "#     score = 0\n",
    "\n",
    "#     while not terminated and not truncated:\n",
    "#         action = env.action_space.sample()\n",
    "#         obs, reward, terminated, truncated , info = env.step(action)\n",
    "#         env.render()\n",
    "#         score += reward\n",
    "#     print(f\"Episode: {episode}  Score {score}\")\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cabbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71257d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(20000, reset_num_timesteps=False, progress_bar=True)\n",
    "model.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
