{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5be620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym\n",
    "import os\n",
    "import random\n",
    "from typing import Any\n",
    "\n",
    "# helpers\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete, Tuple\n",
    "\n",
    "# Stable-baseline\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from environment import KubernetesEnv\n",
    "\n",
    "# Custom\n",
    "from typing import Optional\n",
    "from database.influxdb import InfluxDB\n",
    "import logging\n",
    "from kubernetes import client\n",
    "from utils import setup_logger\n",
    "from prometheus_api_client import PrometheusConnect\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b4f3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = setup_logger(\n",
    "    \"kubernetes_agent\", log_level=os.getenv(\"LOG_LEVEL\", \"INFO\"), log_to_file=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353220fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_pods_ready(\n",
    "    prometheus: PrometheusConnect,\n",
    "    namespace: str,\n",
    "    deployment_name: str,\n",
    "    timeout: int,\n",
    "):\n",
    "    start_time = time.time()\n",
    "    ready_replicas = 0\n",
    "\n",
    "    scope_ready = f\"\"\"\n",
    "        (kube_pod_status_ready{{namespace=\"{namespace}\", condition=\"true\"}} == 1)\n",
    "        and on(pod)\n",
    "        (\n",
    "          label_replace(\n",
    "            kube_pod_owner{{namespace=\"{namespace}\", owner_kind=\"ReplicaSet\"}},\n",
    "            \"replicaset\", \"$1\", \"owner_name\", \"(.*)\"\n",
    "          )\n",
    "          * on(namespace, replicaset) group_left(owner_name)\n",
    "            kube_replicaset_owner{{\n",
    "              namespace=\"{namespace}\", owner_kind=\"Deployment\", owner_name=\"{deployment_name}\"\n",
    "            }}\n",
    "        )\n",
    "    \"\"\"  # noqa: E501\n",
    "    q_desired = f\"\"\"\n",
    "    scalar(\n",
    "      sum(\n",
    "        kube_deployment_spec_replicas{{namespace=\"{namespace}\",\n",
    "        deployment=\"{deployment_name}\"}}\n",
    "      )\n",
    "    )\n",
    "    \"\"\"\n",
    "    q_ready = f\"\"\"\n",
    "      scalar(sum({scope_ready}))\n",
    "    \"\"\"\n",
    "\n",
    "    while time.time() - start_time < timeout:\n",
    "        desired_result = prometheus.custom_query(q_desired)\n",
    "        desired = desired_result[1]\n",
    "        ready_result = prometheus.custom_query(query=q_ready)\n",
    "        ready = ready_result[1]\n",
    "        if ready == desired:\n",
    "            return True, desired, ready\n",
    "        logger.info(\n",
    "            f\"Waiting for pods to be ready: {ready}/{desired}\"\n",
    "        )\n",
    "        time.sleep(1)\n",
    "    return False, desired, ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba23d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metrics_query(\n",
    "    namespace: str,\n",
    "    deployment_name: str,\n",
    "    interval: int = 15,\n",
    "    desired_replicas: int | None = None,\n",
    "    quantile: float = 0.90,\n",
    "    endpoints_method: list[tuple[str, str]] = ((\"/\", \"GET\"), (\"/docs\", \"GET\")),\n",
    ") -> tuple[str, str, str, str, str]:\n",
    "    \"\"\"\n",
    "    Build pod-scoped queries and cap to the youngest desired pods.\n",
    "\n",
    "    We use topk on pod start time to keep only the newest N pods (desired replicas),\n",
    "    so older pods that are still Ready after a scale-down do not contribute.\n",
    "    \"\"\"\n",
    "    # Default to a reasonable cap if desired_replicas is None\n",
    "    pod_window = max(1, desired_replicas or 50)\n",
    "\n",
    "    pod_filter = f\"\"\"\n",
    "        topk({pod_window},\n",
    "          kube_pod_start_time{{\n",
    "            namespace=\"{namespace}\",\n",
    "            pod=~\"{deployment_name}-.*\"\n",
    "          }}\n",
    "          * on(pod) group_left()\n",
    "            (kube_pod_status_ready{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                condition=\"true\"\n",
    "            }} == 1)\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    cpu_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            rate(container_cpu_usage_seconds_total{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                container!=\"\",\n",
    "                container!=\"POD\"\n",
    "            }}[{interval}s])\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    memory_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            container_memory_working_set_bytes{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                container!=\"\",\n",
    "                container!=\"POD\"\n",
    "            }}\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    cpu_limits_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            kube_pod_container_resource_limits{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                resource=\"cpu\",\n",
    "                unit=\"core\"\n",
    "            }}\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    # Query for memory limits\n",
    "    memory_limits_query = f\"\"\"\n",
    "        sum by (pod) (\n",
    "            kube_pod_container_resource_limits{{\n",
    "                namespace=\"{namespace}\",\n",
    "                pod=~\"{deployment_name}-.*\",\n",
    "                resource=\"memory\",\n",
    "                unit=\"byte\"\n",
    "            }}\n",
    "        )\n",
    "        * on(pod) group_left() {pod_filter}\n",
    "        \"\"\"\n",
    "\n",
    "    response_time_query = []\n",
    "    for endpoint, method in endpoints_method:\n",
    "        response_time_query.append(f\"\"\"\n",
    "                1000 *\n",
    "                histogram_quantile(\n",
    "                {quantile},\n",
    "                sum by (le) (\n",
    "                    rate(http_request_duration_seconds_bucket{{\n",
    "                    namespace=\"{namespace}\",\n",
    "                    pod=~\"{deployment_name}-.*\",\n",
    "                    method=\"{method}\",\n",
    "                    path=\"{endpoint}\"\n",
    "                    }}[{interval}s])\n",
    "                )\n",
    "                )\n",
    "            \"\"\"\n",
    "        )\n",
    "    return (\n",
    "        cpu_query,\n",
    "        memory_query,\n",
    "        cpu_limits_query,\n",
    "        memory_limits_query,\n",
    "        response_time_query,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98efa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_metrics(cpu_usage, memory_usage, cpu_limits, memory_limits, response_times, max_response_time):\n",
    "    cpu_percentages = []\n",
    "    memory_percentages = []\n",
    "\n",
    "    cpu_limits_by_pod = {}\n",
    "    memory_limits_by_pod = {}\n",
    "    for item in cpu_limits:\n",
    "        pod = item[\"metric\"][\"pod\"]\n",
    "        limit = float(item[\"value\"][1])\n",
    "        cpu_limits_by_pod[pod] = limit\n",
    "\n",
    "    for item in memory_limits:\n",
    "        pod = item[\"metric\"][\"pod\"]\n",
    "        limit = float(item[\"value\"][1])\n",
    "        memory_limits_by_pod[pod] = limit\n",
    "\n",
    "    for result in cpu_usage:\n",
    "        pod_name = result[\"metric\"].get(\"pod\")\n",
    "        limit = float(cpu_limits_by_pod.get(pod_name))\n",
    "        rate_cores = float(result[\"value\"][1])\n",
    "        cpu_percentage = (rate_cores / limit) * 100\n",
    "        cpu_percentages.append(cpu_percentage)\n",
    "\n",
    "    for result in memory_usage:\n",
    "        pod_name = result[\"metric\"].get(\"pod\")\n",
    "        limit = float(memory_limits_by_pod.get(pod_name))\n",
    "        usage_bytes = float(result[\"value\"][1])\n",
    "        memory_percentage = (usage_bytes / limit) * 100\n",
    "        memory_percentages.append(memory_percentage)\n",
    "\n",
    "    response_time = np.mean(response_times)\n",
    "    response_time_percentage = (response_time / max_response_time) * 100.0\n",
    "    response_time_percentage = min(response_time_percentage, 1000.0)\n",
    "\n",
    "    return cpu_percentages, memory_percentages, response_time_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e35dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(\n",
    "    prometheus: PrometheusConnect,\n",
    "    namespace: str,\n",
    "    deployment_name: str,\n",
    "    interval: int,\n",
    "    replica: int,\n",
    "    max_response_time: float,\n",
    "):\n",
    "    (\n",
    "        cpu_query,\n",
    "        memory_query,\n",
    "        cpu_limits_query,\n",
    "        memory_limits_query,\n",
    "        response_time_query,\n",
    "    )= _metrics_query(\n",
    "        namespace,\n",
    "        deployment_name,\n",
    "        interval=interval,\n",
    "        desired_replicas=replica,\n",
    "    )\n",
    "    cpu_usage_results = prometheus.custom_query(cpu_query)\n",
    "    memory_usage_results = prometheus.custom_query(memory_query)\n",
    "    cpu_limits_results = prometheus.custom_query(cpu_limits_query)\n",
    "    memory_limits_results = prometheus.custom_query(memory_limits_query)\n",
    "\n",
    "    response_time_results = []\n",
    "    for query in response_time_query:\n",
    "        response = prometheus.custom_query(query)\n",
    "        response_time_results.append(float(response[0][\"value\"][1]))\n",
    "\n",
    "    cpu_percentages, memory_percentages, response_time_percentage = process_metrics(\n",
    "        cpu_usage_results,\n",
    "        memory_usage_results,\n",
    "        cpu_limits_results,\n",
    "        memory_limits_results,\n",
    "        response_time_results,\n",
    "        max_response_time,\n",
    "    )\n",
    "\n",
    "    return cpu_percentages, memory_percentages, response_time_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e706895",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KubernetesEnv(Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_replicas: int = 1,\n",
    "        max_replicas: int = 50,\n",
    "        iteration: int = 50,\n",
    "        namespace: str = \"default\",\n",
    "        deployment_name: str = \"default\",\n",
    "        min_cpu: float = 20,\n",
    "        min_memory: float = 20,\n",
    "        max_cpu: float = 90,\n",
    "        max_memory: float = 90,\n",
    "        max_response_time: float = 100.0,\n",
    "        timeout: int = 120,\n",
    "        wait_time: int = 30,\n",
    "        verbose: bool = False,\n",
    "        logger: Optional[logging.Logger] = None,\n",
    "        influxdb: Optional[InfluxDB] = None,\n",
    "        prometheus_url: str = \"http://localhost:1234/prom\",\n",
    "        metrics_endpoints_method: list[tuple[str, str]] = (\n",
    "            (\"/\", \"GET\"),\n",
    "            (\"/docs\", \"GET\"),\n",
    "        ),\n",
    "        metrics_interval: int = 15,\n",
    "        metrics_quantile: float = 0.90,\n",
    "        max_scaling_retries: int = 1000,\n",
    "    ):\n",
    "        self.api = client.AppsV1Api()\n",
    "        self.namespace = namespace\n",
    "        self.deployment_name = deployment_name\n",
    "        self.prometheus = PrometheusConnect(\n",
    "            url=prometheus_url,\n",
    "            disable_ssl=True,\n",
    "        )\n",
    "        self.timeout = timeout\n",
    "        self.wait_time = wait_time\n",
    "        self.metrics_interval = metrics_interval\n",
    "        self.metrics_quantile = metrics_quantile\n",
    "        self.metrics_endpoints_method = metrics_endpoints_method\n",
    "\n",
    "        self.logger = logger\n",
    "\n",
    "        self.action_space = Discrete(100)\n",
    "        self.observation_space = Box(low=0.0, high=100.0, shape=(17,), dtype=np.float32)\n",
    "        self.state = 0\n",
    "        self.iteration = 50\n",
    "        self.min_replicas: int = min_replicas\n",
    "        self.max_replicas: int = max_replicas\n",
    "        self.range_replicas: int = max(1, self.max_replicas - self.min_replicas)\n",
    "        self.max_response_time: float = max_response_time\n",
    "        self.min_cpu: float = min_cpu\n",
    "        self.min_memory: float = min_memory\n",
    "        self.max_cpu: float = max_cpu\n",
    "        self.max_memory: float = max_memory\n",
    "\n",
    "    def step(self, action: int):\n",
    "        replica = action * self.range_replicas // 100 + self.min_replicas\n",
    "        logger.info(f\"Set state to {self.state} replicas\")\n",
    "\n",
    "        cpu, memory, response_time = self.scale_and_get_metrics(replica)\n",
    "\n",
    "        cpu, memory = self.calculate_distance(cpu, memory)\n",
    "\n",
    "        reward = self.calculate_reward(action, cpu, memory, response_time)\n",
    "\n",
    "        observation = self.observation(action, response_time, cpu, memory)\n",
    "\n",
    "        return observation, reward, False, False, {}\n",
    "\n",
    "    def scale_and_get_metrics(self, replica: int) -> None:\n",
    "        self.api.patch_namespaced_deployment_scale(\n",
    "            name=self.deployment_name,\n",
    "            namespace=self.namespace,\n",
    "            body={\"spec\": {\"replicas\": replica}},\n",
    "        )\n",
    "        logger.info(f\"Scaled to {replica} replicas\")\n",
    "        _, _, _ = wait_for_pods_ready(\n",
    "            prometheus=self.prometheus,\n",
    "            deployment_name=self.deployment_name,\n",
    "            desired_replicas=replica,\n",
    "            namespace=self.namespace,\n",
    "            timeout=self.timeout,\n",
    "            logger=self.logger,\n",
    "        )\n",
    "        cpu, memory, response_time = get_metrics(\n",
    "            prometheus=self.prometheus,\n",
    "            namespace=self.namespace,\n",
    "            deployment_name=self.deployment_name,\n",
    "            interval=self.metrics_interval,\n",
    "            replica=replica,\n",
    "            max_response_time=self.max_response_time,\n",
    "        )\n",
    "        return cpu, memory, response_time\n",
    "\n",
    "    def calculate_reward(self, action: int, cpu: float, memory: float, response_time: float) -> float:\n",
    "\n",
    "        cpu_penalty = min(cpu * cpu, 1.0)\n",
    "        memory_penalty = min(memory * memory, 1.0)\n",
    "\n",
    "        RESPONSE_TIME_HIGH_THRESHOLD = 80.0\n",
    "        RESPONSE_TIME_VIOLATION_THRESHOLD = 100.0\n",
    "        MAX_RESPONSE_PENALTY = 2.0\n",
    "        if response_time <= RESPONSE_TIME_HIGH_THRESHOLD:\n",
    "            response_time_penalty = 0.0\n",
    "        elif response_time <= RESPONSE_TIME_VIOLATION_THRESHOLD:\n",
    "            response_time_penalty = (response_time - RESPONSE_TIME_HIGH_THRESHOLD) / (\n",
    "                RESPONSE_TIME_VIOLATION_THRESHOLD - RESPONSE_TIME_HIGH_THRESHOLD\n",
    "            )\n",
    "        else:\n",
    "            over = (\n",
    "                response_time - RESPONSE_TIME_VIOLATION_THRESHOLD\n",
    "            ) / RESPONSE_TIME_VIOLATION_THRESHOLD\n",
    "            response_time_penalty = 1.0 + over\n",
    "\n",
    "        response_time_penalty = max(0.0, min(response_time_penalty, MAX_RESPONSE_PENALTY))\n",
    "\n",
    "        cost_pen = action / 100.0\n",
    "        reward = 1.0 - 2.0 * (\n",
    "            cpu_penalty + memory_penalty + response_time_penalty + cost_pen\n",
    "        )\n",
    "        return max(reward, -1.0)\n",
    "\n",
    "    def calculate_distance(self,cpu: float, memory: float) -> tuple[float, float]:\n",
    "        cpu_distance = (\n",
    "            (self.min_cpu - cpu) if cpu < self.min_cpu else (cpu - self.max_cpu)\n",
    "        )\n",
    "        cpu_bandwidth = self.max_cpu - self.min_cpu\n",
    "        cpu_normalized = cpu_distance / cpu_bandwidth\n",
    "        cpu = cpu_normalized * 100.0\n",
    "\n",
    "\n",
    "        memory_distance = (\n",
    "            (self.min_memory - memory)\n",
    "            if memory < self.min_memory\n",
    "            else (memory - self.max_memory)\n",
    "        )\n",
    "        memory_bandwidth = self.max_memory - self.min_memory\n",
    "        memory_normalized = memory_distance / memory_bandwidth\n",
    "        memory = memory_normalized * 100.0\n",
    "\n",
    "        return cpu, memory\n",
    "\n",
    "    def observation(self, action: int, response_time: float, cpu: float, memory: float):\n",
    "        return np.array(\n",
    "            [\n",
    "                action,\n",
    "                cpu,\n",
    "                memory,\n",
    "                response_time,\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53faf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = KubernetesEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c064f75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([73.97517  , 21.329208 , 55.635826 , 81.22282  , 27.32877  ,\n",
       "       15.381341 ,  4.8464737, 51.462143 , 30.69403  , 62.61727  ,\n",
       "        5.507853 , 24.718836 , 92.99492  , 83.80141  , 27.520195 ,\n",
       "       42.68573  , 95.34111  ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f9c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(45)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
