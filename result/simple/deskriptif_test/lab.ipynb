{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968615b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SCRIPT_DIR = (\n",
    "    os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    ")\n",
    "PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "CHARTS_DIR = os.path.join(SCRIPT_DIR, \"charts\")\n",
    "NUM_RUNS = 10\n",
    "\n",
    "METRICS = [\n",
    "    (\"../data/pod_10\", \"response_time\", \"Response Time (ms)\", 10),\n",
    "    (\"../data/pod_10\", \"replica\", \"Replica Count\", 10),\n",
    "    (\"../data/pod_10\", \"cpu\", \"CPU Usage\", 10),\n",
    "    (\"../data/pod_10\", \"memory\", \"Memory Usage\", 10),\n",
    "    (\"../data/pod_20\", \"response_time\", \"Response Time (ms)\", 20),\n",
    "    (\"../data/pod_20\", \"replica\", \"Replica Count\", 20),\n",
    "    (\"../data/pod_20\", \"cpu\", \"CPU Usage\", 20),\n",
    "    (\"../data/pod_20\", \"memory\", \"Memory Usage\", 20),\n",
    "]\n",
    "\n",
    "PLOT_CONFIG = {\n",
    "    \"cpu\": {\"metric_name\": \"CPU Usage\", \"ylabel\": \"CPU (%)\", \"ylim\": 100, \"threshold\": None},\n",
    "    \"memory\": {\n",
    "        \"metric_name\": \"Memory Usage\",\n",
    "        \"ylabel\": \"Memory (%)\",\n",
    "        \"ylim\": 100,\n",
    "        \"threshold\": None,\n",
    "    },\n",
    "    \"response_time\": {\n",
    "        \"metric_name\": \"Response Time\",\n",
    "        \"ylabel\": \"Response Time (ms)\",\n",
    "        \"ylim\": 1300,\n",
    "        \"threshold\": 1000,\n",
    "    },\n",
    "    \"replica\": {\n",
    "        \"metric_name\": \"Desired Replicas\",\n",
    "        \"ylabel\": \"Replicas\",\n",
    "        \"ylim\": 20,\n",
    "        \"threshold\": None,\n",
    "    },\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf104b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_folder(folder: str) -> str:\n",
    "    normalized = folder.replace(\"\\\\\", \"/\")\n",
    "    while normalized.startswith(\"../\"):\n",
    "        normalized = normalized[3:]\n",
    "    return normalized.strip(\"/\")\n",
    "\n",
    "\n",
    "def load_influx_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, skiprows=3, comment=\"#\")\n",
    "    df = df.drop(columns=[c for c in df.columns if \"Unnamed\" in str(c)], errors=\"ignore\")\n",
    "    df[\"_time\"] = pd.to_datetime(df[\"_time\"], errors=\"coerce\")\n",
    "    df[\"_value\"] = pd.to_numeric(df[\"_value\"], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_all_runs(base_path, metric_folder, num_runs):\n",
    "    runs = {}\n",
    "    rel = base_path[5:] if base_path.startswith(\"data/\") else base_path\n",
    "    for i in range(1, num_runs + 1):\n",
    "        path = os.path.join(DATA_DIR, rel, metric_folder, f\"{i}.csv\")\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                runs[i] = load_influx_csv(path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {path}: {e}\")\n",
    "    return runs\n",
    "\n",
    "\n",
    "def calculate_run_statistics(runs, metric_name):\n",
    "    stats = []\n",
    "    for run_id, df in runs.items():\n",
    "        if \"deployment\" not in df.columns or \"_value\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        hpa_data = df[df[\"deployment\"] == \"hpa-flask-app\"][\"_value\"]\n",
    "        rl_data = df[df[\"deployment\"] == \"test-flask-app\"][\"_value\"]\n",
    "        if len(hpa_data) == 0 or len(rl_data) == 0:\n",
    "            continue\n",
    "\n",
    "        hpa_mean = hpa_data.mean()\n",
    "        rl_mean = rl_data.mean()\n",
    "        hpa_std = hpa_data.std()\n",
    "        rl_std = rl_data.std()\n",
    "        hpa_max = hpa_data.max()\n",
    "        rl_max = rl_data.max()\n",
    "\n",
    "        improvement = ((hpa_mean - rl_mean) / hpa_mean * 100) if hpa_mean != 0 else 0\n",
    "        stability_score = 1 / (hpa_std + rl_std + 1)\n",
    "\n",
    "        stats.append(\n",
    "            {\n",
    "                \"Run\": run_id,\n",
    "                \"Metric\": metric_name,\n",
    "                \"HPA Mean\": hpa_mean,\n",
    "                \"RL Mean\": rl_mean,\n",
    "                \"HPA Std\": hpa_std,\n",
    "                \"RL Std\": rl_std,\n",
    "                \"HPA Max\": hpa_max,\n",
    "                \"RL Max\": rl_max,\n",
    "                \"Improvement (%)\": improvement,\n",
    "                \"Data Points\": len(df),\n",
    "                \"Stability\": stability_score,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "def get_recommendation_score(stats_df, metric_type):\n",
    "    df = stats_df.copy()\n",
    "\n",
    "    if len(df) > 1:\n",
    "        df[\"Improvement_norm\"] = (df[\"Improvement (%)\"] - df[\"Improvement (%)\"].min()) / (\n",
    "            df[\"Improvement (%)\"].max() - df[\"Improvement (%)\"].min() + 0.001\n",
    "        )\n",
    "        df[\"Stability_norm\"] = (df[\"Stability\"] - df[\"Stability\"].min()) / (\n",
    "            df[\"Stability\"].max() - df[\"Stability\"].min() + 0.001\n",
    "        )\n",
    "    else:\n",
    "        df[\"Improvement_norm\"] = 0.5\n",
    "        df[\"Stability_norm\"] = 0.5\n",
    "\n",
    "    if metric_type in [\"Response Time\", \"CPU\", \"Memory\"]:\n",
    "        df[\"Score\"] = (0.7 * df[\"Improvement_norm\"]) + (0.3 * df[\"Stability_norm\"])\n",
    "    else:\n",
    "        df[\"Score\"] = (0.5 * df[\"Improvement_norm\"]) + (0.5 * df[\"Stability_norm\"])\n",
    "\n",
    "    return df.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "\n",
    "def metric_type_from_label(metric_label: str) -> str:\n",
    "    if \"Response Time\" in metric_label:\n",
    "        return \"Response Time\"\n",
    "    if \"CPU\" in metric_label:\n",
    "        return \"CPU\"\n",
    "    if \"Memory\" in metric_label:\n",
    "        return \"Memory\"\n",
    "    return \"Replica\"\n",
    "\n",
    "\n",
    "def extract_recommended_runs(stats_df, metric_type):\n",
    "    if stats_df.empty or \"Run\" not in stats_df.columns:\n",
    "        return []\n",
    "    ranked = get_recommendation_score(stats_df, metric_type)\n",
    "    return ranked[\"Run\"].astype(int).tolist()\n",
    "\n",
    "\n",
    "def plot_single_run_high_quality(\n",
    "    df,\n",
    "    run_id,\n",
    "    metric_name,\n",
    "    ylabel,\n",
    "    ylim,\n",
    "    pod_label,\n",
    "    recommendation_tag=\"\",\n",
    "    threshold=None,\n",
    "    save_path=None,\n",
    "):\n",
    "    deploy_a = \"hpa-flask-app\"\n",
    "    deploy_b = \"test-flask-app\"\n",
    "    colors = {deploy_a: \"#0072B2\", deploy_b: \"#D55E00\"}\n",
    "\n",
    "    plot_df = df.copy()\n",
    "    min_time = plot_df[\"_time\"].min()\n",
    "    plot_df[\"_elapsed_min\"] = (plot_df[\"_time\"] - min_time).dt.total_seconds() / 60\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "    for deploy in [deploy_a, deploy_b]:\n",
    "        subset = plot_df[plot_df[\"deployment\"] == deploy].sort_values(\"_elapsed_min\")\n",
    "        label = \"HPA\" if deploy == deploy_a else \"RL Agent\"\n",
    "        ax.plot(\n",
    "            subset[\"_elapsed_min\"],\n",
    "            subset[\"_value\"],\n",
    "            label=label,\n",
    "            color=colors[deploy],\n",
    "            linewidth=2,\n",
    "            alpha=0.9,\n",
    "        )\n",
    "\n",
    "    ax.set_ylabel(ylabel, fontsize=13)\n",
    "    ax.set_xlabel(\"Elapsed Time (minutes)\", fontsize=13)\n",
    "    ax.set_title(\n",
    "        f\"{metric_name} - Run #{run_id} ({pod_label})\",\n",
    "        fontsize=15,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax.set_ylim(0, ylim)\n",
    "\n",
    "    if \"Replicas\" in metric_name:\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "\n",
    "    if threshold:\n",
    "        ax.axhline(\n",
    "            y=threshold,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            alpha=0.7,\n",
    "            label=f\"Threshold ({threshold}ms)\",\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"upper right\", fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"  Saved: {save_path}\")\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2eef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_by_key = {}\n",
    "stats_by_key = {}\n",
    "recommendations_by_key = {}\n",
    "\n",
    "print(\"Loading test runs from all steps...\")\n",
    "for folder, metric_dir, metric_label, pod in METRICS:\n",
    "    scenario_folder = normalize_folder(folder)\n",
    "    key = (pod, metric_dir)\n",
    "    runs = load_all_runs(scenario_folder, metric_dir, NUM_RUNS)\n",
    "    stats_df = calculate_run_statistics(runs, metric_label)\n",
    "    metric_type = metric_type_from_label(metric_label)\n",
    "    recommendations = extract_recommended_runs(stats_df, metric_type)\n",
    "\n",
    "    runs_by_key[key] = runs\n",
    "    stats_by_key[key] = stats_df\n",
    "    recommendations_by_key[key] = recommendations\n",
    "\n",
    "    print(f\"  pod_{pod}/{metric_dir}: {len(runs)} runs\")\n",
    "\n",
    "print(\"\\nCreating charts for all runs (recommended runs are tagged)...\")\n",
    "for folder, metric_dir, metric_label, pod in METRICS:\n",
    "    key = (pod, metric_dir)\n",
    "    runs = runs_by_key.get(key, {})\n",
    "    top_runs = recommendations_by_key.get(key, [])[:3]\n",
    "    recommended_set = set(top_runs)\n",
    "\n",
    "    if not runs:\n",
    "        continue\n",
    "\n",
    "    output_dir = os.path.join(CHARTS_DIR, f\"pod_{pod}\", metric_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    config = PLOT_CONFIG[metric_dir]\n",
    "    for run_id in sorted(runs.keys()):\n",
    "        recommendation_tag = \"recommended\" if run_id in recommended_set else \"\"\n",
    "        recommendation_suffix = \"_recommended\" if recommendation_tag == \"recommended\" else \"\"\n",
    "        save_path = os.path.join(\n",
    "            output_dir,\n",
    "            f\"deskriptif_{metric_dir}_run_{run_id}{recommendation_suffix}.png\",\n",
    "        )\n",
    "        plot_single_run_high_quality(\n",
    "            runs[run_id],\n",
    "            run_id,\n",
    "            config[\"metric_name\"],\n",
    "            config[\"ylabel\"],\n",
    "            config[\"ylim\"],\n",
    "            f\"Pod {pod}\",\n",
    "            recommendation_tag,\n",
    "            config[\"threshold\"],\n",
    "            save_path,\n",
    "        )\n",
    "\n",
    "print(\"\\nDeskriptif chart generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09febec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deskriptif_summary(stats_by_key, out_dir):\n",
    "    \"\"\"Generate LaTeX table summarizing mean HPA/RL values per scenario, pod, and metric.\"\"\"\n",
    "    rows = []\n",
    "    for key, df in stats_by_key.items():\n",
    "        pod, metric_dir = key\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        for _, r in df.iterrows():\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"pod\": (f\"pod_{pod}\").replace(\"_\", \"\\\\_\"),\n",
    "                    \"metric_dir\": metric_dir,\n",
    "                    \"metric_label\": r.get(\"Metric\", metric_dir),\n",
    "                    \"hpa_mean\": r.get(\"HPA Mean\", np.nan),\n",
    "                    \"rl_mean\": r.get(\"RL Mean\", np.nan),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not rows:\n",
    "        print(\"No descriptive stats available to summarize.\")\n",
    "        return None\n",
    "\n",
    "    df_all = pd.DataFrame(rows)\n",
    "\n",
    "    # Group by all scenario dimensions + metric, averaging across runs\n",
    "    summary = (\n",
    "        df_all.groupby([\"pod\", \"metric_label\"])[[\"hpa_mean\", \"rl_mean\"]]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values([\"pod\", \"metric_label\"])\n",
    "    )\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # create overall per-metric mean across all scenarios\n",
    "    overall = (\n",
    "        df_all.groupby([\"metric_label\"])[[\"hpa_mean\", \"rl_mean\"]]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .sort_values(\"metric_label\")\n",
    "    )\n",
    "\n",
    "    # --- Overall per-metric table ---\n",
    "    overall_path = os.path.join(out_dir, \"analisis_deskriptif_rangkuman.tex\")\n",
    "    lines = []\n",
    "    lines.append(\"\\\\begin{table}[ht]\")\n",
    "    lines.append(\"  \\\\centering\")\n",
    "    lines.append(\"  \\\\caption{Rangkuman Rata-rata per Metrik (seluruh skenario)}\\\\label{tab:deskriptif-summary-per-metric}\")\n",
    "    lines.append(\"  \\\\begin{tabular}{lcc}\")\n",
    "    lines.append(\"    \\\\toprule\")\n",
    "    lines.append(r\"    Metrik & Rata-rata HPA & Rata-rata RL \\\\\")\n",
    "    lines.append(\"    \\\\midrule\")\n",
    "\n",
    "    for _, r in overall.iterrows():\n",
    "        metric = r[\"metric_label\"]\n",
    "        hpa_str = f\"{r['hpa_mean']:.3f}\" if not pd.isna(r[\"hpa_mean\"]) else \"-\"\n",
    "        rl_str = f\"{r['rl_mean']:.3f}\" if not pd.isna(r[\"rl_mean\"]) else \"-\"\n",
    "        lines.append(f\"    {metric} & {hpa_str} & {rl_str} \" + r\"\\\\\")\n",
    "\n",
    "    lines.append(\"    \\\\bottomrule\")\n",
    "    lines.append(\"  \\\\end{tabular}\")\n",
    "    lines.append(\"\\\\end{table}\")\n",
    "\n",
    "    with open(overall_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "    print(f\"Saved overall per-metric LaTeX: {overall_path}\")\n",
    "\n",
    "    # --- Per-metric per-scenario tables (optional detailed tables) ---\n",
    "    for metric_label, group_df in summary.groupby(\"metric_label\"):\n",
    "        safe = (\n",
    "            metric_label.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")\n",
    "        )\n",
    "        path = os.path.join(out_dir, f\"analisis_deskriptif_{safe}.tex\")\n",
    "        lines = []\n",
    "        lines.append(\"\\\\begin{table}[ht]\")\n",
    "        lines.append(\"  \\\\centering\")\n",
    "        lines.append(f\"  \\\\caption{{Rata-rata {metric_label} per Skenario}}\\\\label{{tab:deskriptif-{safe}}}\")\n",
    "        lines.append(\"  \\\\begin{tabular}{llllcc}\")\n",
    "        lines.append(\"    \\\\toprule\")\n",
    "        lines.append(r\"    Pod & Metrik & Rata-rata HPA & Rata-rata RL \\\\\")\n",
    "        lines.append(\"    \\\\midrule\")\n",
    "\n",
    "        prev_group = None\n",
    "        for _, r in group_df.iterrows():\n",
    "            pod = r[\"pod\"]\n",
    "            metric = r[\"metric_label\"]\n",
    "            hpa_str = f\"{r['hpa_mean']:.3f}\" if not pd.isna(r[\"hpa_mean\"]) else \"-\"\n",
    "            rl_str = f\"{r['rl_mean']:.3f}\" if not pd.isna(r[\"rl_mean\"]) else \"-\"\n",
    "\n",
    "            group = (pod)\n",
    "            if prev_group is not None and group != prev_group:\n",
    "                lines.append(\"    \\\\midrule\")\n",
    "            prev_group = group\n",
    "\n",
    "            lines.append(f\"    {pod} & {metric} & {hpa_str} & {rl_str} \" + r\"\\\\\")\n",
    "\n",
    "        lines.append(\"    \\\\bottomrule\")\n",
    "        lines.append(\"  \\\\end{tabular}\")\n",
    "        lines.append(\"\\\\end{table}\")\n",
    "\n",
    "        with open(path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(lines))\n",
    "\n",
    "        print(f\"Saved per-metric LaTeX: {path}\")\n",
    "\n",
    "    return overall_path\n",
    "\n",
    "try:\n",
    "    SUMMARY_OUT = os.path.join(\"tables\")\n",
    "    generate_deskriptif_summary(stats_by_key, SUMMARY_OUT)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate descriptive summary: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
